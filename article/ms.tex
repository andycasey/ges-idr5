%% Copyright 2016 Andrew R. Casey.  All rights reserved.

\documentclass[preprint]{aastex}

\usepackage{amsmath}
\usepackage{bm}

\IfFileExists{vc.tex}{\input{vc.tex}}{\newcommand{\githash}{UNKNOWN}\newcommand{\giturl}{UNKNOWN}}

\newcommand{\acronym}[1]{{\small{#1}}}

\newcommand{\ges}{\acronym{GES}}
\newcommand{\project}[1]{\textsl{#1}}

\newcommand{\gaiaeso}{\project{Gaia-ESO}}

\newcommand{\teff}{T_{\mathrm{eff}}}
\newcommand{\logg}{\log g}
\newcommand{\feh}{[\mathrm{Fe/H}]}
\newcommand{\normal}[2]{\mathcal{N}\left(#1, #2\right)}

\begin{document}
\title{The Gaia-ESO Survey: Combined estimates from multiple stellar analysis pipelines}

\author{
    Andrew~R.~Casey\altaffilmark{1},
   }


\begin{abstract}
Some stuff
\end{abstract}


\keywords{stars: fundamental parameters --- stars: abundances
% Clear rest of page
\clearpage}


\section{Introduction} 
\label{sec:introduction}

% In terms of telescope time cost, GES is the largest by far.

% Huge time allocation to do heaps of cool shit.

% Analyse all spectral types

% Provides a ground-based complement to the Gaia mission.

% Huge collaboration, most of which were in-fighting previously.

% Through GES they started working together.... 

% Experts in different fields, etc.

% Multiple analysis strategy.


\section{Data}
\label{sec:data}

% Description of the setups

% Summary of the ges type and SNR of the data.

% Description of target sources included: MW, Clusters, CoRot, K2, etc.

% Blind test.

\section{Methods}
\label{sec:methods}

% Multiple analysis strategy: why

% primary advantage and disadvantage to multiple analysis strategy

% Description of two parameter rounds: stellar parameters, abundances

% What's reported by each node? Flags? Consistency of flags?

% Description of pipelines in appendix 

% Commonalities between nodes: model atmospheres, line list, normalized spectra, initial params, spectrum library, xi calibration if necessary.



% EW method and comparisons -- into an appendix?.


% What do the nodes provide? Most parameters, flags, etc.

% CONTINUE from here:
% Stellar parameter determination

\subsection{Technical flags}
\label{sec:technical-flags}

In the fifth \gaiaeso\ data release we required all analysis nodes to either
supply results for a spectrum, or to provide a technical flag (column \texttt{TECH})
explaining no results were reported. We list all relevant technical flags and their
interpretation in Appendix \ref{app:flags}. There is a large variation in specificity 
between flags. For example, some flags indicate that an analysis pipeline did not 
converge (for whatever reason), whereas some flags indicate a result is uncertain 
due to a data reduction issue producing a `picket-fence'-like pattern in the 
spectrum. The former is provided automatically and may reflect uncertainty 
\emph{only} in the results from the node who reported the flag. However, the latter 
example was presumably flagged because a human looked at the data, and this flag
should raise uncertainty in \emph{all} results using the same spectrum.  Similar 
examples can be drawn for flags describing issues intrinsic to the star (e.g., high
rotational velocity), and not just a single spectrum.  For these reasons, there are 
situations where technical flags ought to be propagated to other results derived from 
the same spectrum, or other results reported for the same star.


We investigated usage patterns between nodes to evaluate whether any flag propagation
was warranted. A heatmap of common flag usage for WG11 nodes is shown in Figure
\ref{fig:wg11-flag-heatmap}.
% TODO: Include heat maps of other WGs?
If all nodes reported the same flags under common conditions, then the node-to-node 
segments in Figure \ref{fig:wg11-flag-heatmap} would show 1-to-1 correlations. This is 
not the case; we find the flag usage to be mostly uncorrelated between different nodes. 
When one node reports a \emph{specific} flag indicating an issue that is intrinsic to 
the spectrum or star, it is highly unlikely that \emph{any} other node has reported the 
same problem.\footnote{In some sense, this is a consequence of having `many eyes on the 
data'.} The most significant correlations are seen in the usage of non-descript flags,
where nodes will report that the derived stellar parameters are outside of the range 
where they consider their results to be `physically realistic' (e.g., outside of a grid
or the effective temperature $\teff$ is too high or low to be reliable). 


Based on our examination of technical flag usage between nodes, we assigned every node 
result with a boolean flag indicating whether the supplied node results ($\teff$, $\logg$,
$\feh$) should be used in the homogenisation process.\footnote{This flag is only present
in an internal PostgreSQL database, which also includes the propagated flag explaining
why a record should not be used in the homogenisation process, and which record the flag
originated from.}  We did not use any results that were marked with any of the technical
flags listed in Table \ref{tab:wg11-node-specific-flags-failing-qc}.  We further discarded 
results flagged as having $v\sin{i} > 20$~km~s$^{-1}$ (flags 11020 and 11050) unless the 
reporting node was OACT, as our examination (over multiple data releases) revealed that 
the OACT node can properly account for fast rotating stars, whereas some nodes may not 
even detect the presence of significant rotation. 


We discarded results from IACAIP that were flagged to be an incomplete spectrum or 
missing wavelengths (flag 10105), as IACAIP seemingly used this flag extensively to 
indicate \emph{any} problem with their own analysis.  For all other nodes that report
flag 10105, we propagated this flag to all results derived from the same spectrum,
and discarded those node estimates.  However, if \emph{any} node reported one of the
data reduction issues in Table \ref{tab:wg11-spectrum-specific-flags-failing-qc}
(e.g., saturated or broken spectrum, poor sky subtraction) then we propagated this 
flag to other results derived from the same spectrum, and discarded those results.
For stars with multiple observations, we were also able to identify situations where 
all nodes reported significantly different stellar parameters, all in the same 
systematic direction (e.g., $\teff$ values reported by all nodes were $\sim$300~K for 
one spectrum). In all cases, these systematic differences arose from archival spectra. 
A visual inspection of these spectra revealed substantial data reduction issues, and 
therefore we discarded all results derived from these problematic spectra.  Finally, 
we discarded \emph{all} node results for a star (matched by \texttt{CNAME}) if any of 
the spectra were flagged -- by any node -- as being a suspected multiple stellar system 
(10320, 13027, or 20020). The total fraction of WG11 records that were discarded due to
flags, including propagated flags, is 23.7\% (11,553 of 48,720), however only 580 unique 
stars analysed by WG11 had all results discarded due to flag usage (of 4,941; 11.7\%).


%FIGURE: wg11-flag-heatmap: Show 2D matrix heatmap of flag usage by WG11 nodes

%TABLE IN APPENDIX \label{app:flags}: <no-name>: List of all relevant flags and their meaning.

%TABLE IN APPENDIX \label{app:flags}: wg11-node-specific-flags-failing-qc: List of node-specific flags that caused us to discard results. Note in the caption the conditions for OACT & vsini, as well as IACAIP with missing wavelengths.

%TABLE IN APPEDIX \label{app:flags}: wg11-spectrum-specific-flags-failing-qc: List of spectrum-specific flags that caused us to discard results. Note in the caption that we did something unusual for IACAIP.
 


% Stellar parameter homogenisation
\subsection{Stellar parameter homogenisation}
\label{sec:stellar-parameter-homogenisation}


\noindent{}We make the following assumptions when constructing a model for homogenisation:

\begin{enumerate}
    \item   \emph{The uncertainties reported by each node are incorrect.}
            This assumption is based on a cursory examination of the data:
            some nodes do not provide uncertainties in stellar parameters, and
            the uncertainties reported by some nodes could vary in percent by
            two orders of magnitude. Therefore, we assume that each node only
            provides a point estimate for astrophysical parameters.

    \item   \emph{Every node provides a biased estimate of stellar parameters.}
            We assume that every node has an unknown bias $\beta$ (offset) in each
            astrophysical parameter, 

            \begin{eqnarray}
                \mu_{true} = \mu_{node} + \beta_{node} + \epsilon
            \end{eqnarray}

            \noindent{}where $\epsilon$ represents the noise in the estimate. As per
            assumption (1), we assert that the distribution of $\epsilon$ is unknown
            on a per-node basis.

    \item   \emph{The unbiased stellar parameters from any two nodes are correlated.}
            After accounting for individual node biases, we assume that the 
            stellar parameters reported by any two nodes will be correlated
            to some degree. The level of correlation between two nodes is unknown.
            This assumption arises from commonalities that led to the estimates from
            each node. Specifically: all nodes are instructed to use a common list 
            of line transitions, the same MARCS model photospheres, some nodes employ
            similar methods (e.g., spectral synthesis), and all nodes derive their 
            estimates from the same data (in some instances, even the same 
            continuum-normalized spectra are used).

            We note that even two nodes with identical methods are not guaranteed
            to be perfectly correlated, because the \emph{choice} of which transitions
            or spectral regions to use is not prescribed.

    \item   \emph{There are gaps in the data.} 
            No spectrum is guaranteed to have stellar parameters reported by all
            nodes. This assumption is a consequence of the different expertise in
            \gaiaeso: no node or method is expert across the entire range of parameter
            space explored. If stellar parameters are not reported, then the reason 
            must be specified through a \texttt{TECH} flag (see Section \ref{sec:flags}
            on flag usage).

    \item   \emph{Random uncertainties in stellar parameters will increase with
            decreasing S/N ratio.}

    \item   \emph{All `well-studied' stars have noisy measurement, and their values
            are not known with infinite precision.} But we assume they are unbiased!

    \item   \emph{The systematic uncertainty will vary across parameter space.}

    \item   \emph{All stellar parameter estimates are normally-distributed draws about 
            the true astrophysical value.}

    \item   \emph{The effective temperature $\teff$, surface gravity $\logg$, and
            metallicity $\feh$ for a single star are uncorrelated.}
            This assumption is provably incorrect, and is made for practical reasons
            only. In Section \ref{sec:scaling-the-model} we describe how the number
            of model parameters scales, and in Section \ref{sec:model-parameterisation}
            we explain how the inference problem itself is numerically unstable.
            Including correlations in stellar parameters would require a constant
            correlation term $\rho$, or modeling $\rho$ as a function of the stellar
            parameters. In either case, the number of model parameters that require
            MCMC will immediately increase by at least factor of three (e.g.,
            $\gtrsim5{,}000$ parameters), amplifying numerical stability issues and
            substantially increasing the requisite number of MCMC samples. For these
            reasons, simultaneously modeling the correlation between stellar
            parameters has been left for an extension of this work.
 
\end{itemize}

Given these assumptions the model we adopt is
\begin{eqnarray}
    \mu_{bm} = \normal{\mu_{true}}{\sigma_{bm}} 
\end{eqnarray} 


% List of the model parameters

We list the nomenclature and descriptions for all model parameters in Table 
\ref{tab:model-parameters}. The number of total model parameters is given by
\begin{eqnarray}
    N_{parameters} = N_{calibrators} + N_{missing} + 6N_{nodes} + \frac{N_{nodes}!}{2(N_{nodes} - 2)!}
\end{eqnarray}
where $N_{calibrators}$ accounts for the true values of the calibrators, the
$N_{missing}$ models the missing data points for the calibrator spectra, the
$6N_{nodes}$ summarises parameters that model biases, as well as systematic
and random uncertainties. The final term $\frac{N_{nodes}!}{2(N_{nodes} - 2)!}$
is the number of correlation coefficients between all nodes.\footnote{
This inference problem is numerically unstable, as there are many model parameters
AND TODO
we include the Cholesky decomposition $L$ of the $N \times N$ correlation matrix $\rho$
as model parameters instead of directly modeling the $\rho$ terms.  The correlation
matrix can be reconstructed by $\rho = (IL)\cdot(IL)^T$ where $I$ is the $N \times N$
identity matrix. This parameterisation helps ensure that the resulting covariance
matrix $\mathbf{\Sigma}$ is positive semi-definite, and can therefore be used for
sampling.} 

% Priors


We implemented the model in \texttt{Stan}, a probabilistic programming
language, and the code describing our parameterisation is included as Appendix \ref{
appendix:stan-code}. We initialised the model with $\mu_{true}$ at the central
calibrator values, and set XXXXXX, with no correlation between any nodes. We
optimized all parameters in \texttt{Stan} using the Broyden-Fletcher-Goldfarb-Shanno
algorithm. The optimized point was used as an initialisation point for Markov Chain
Monte-Carlo (MCMC) sampling.

N chains, thinning, N iterations. Convergence criteria.
 
 




% Abundance determination

% Abundance homogenisation


\section{Discussion}
\label{sec:discussion}

% Is the multiple-analysis strategy justified?
% --> Take any one node, infer their uncertainties as a function of SNR using point-wise correlations (e.g., no benchmarks).
% --> What goes wrong for each node?

% Precision, accuracy, systematics.
% EW


% Analysis choices: we give same line list but not all are used.
%                   or how they are used.

 



\section{Conclusions}
\label{sec:conclusions}

% This should be a "lessons learned" list.

% The final paragraph should contain a succint forward outlook of how good or bad an idea this was.


\end{document}
